\section{Implementación}
\noindent\textbf{Nota:} Esta sección solo se describe la implementación y funcionamiento de los
módulos principales. El código fuente de dichas clases se proporciona en la
\hyperref[sec:Anexos]{sección de Anexos} (página \pageref{sec:Anexos}).

\subsection{Converión de audio a texto}

Para realizar la transcripcion de audio a texto se utilizo el dictado por vos del teclado de
Google. Luego, con la finalidad de identificar los hablantes, se proporciono el
texto transcripto a ChatGPT utilizando el siguiente prompt:

\begin{verbatim}
Identifica y separa los diálogos entre el Agente y 
el Cliente en el siguiente texto:
<insertar el texto>
Dame el resultado en el formato 'Agente:' y 'Cliente:'
\end{verbatim}

\subsection{Estructura del codigo}
El código está estructurado de la siguiente manera:

\begin{tcolorbox}[colback=gray!10, colframe=gray!80, sharp corners, boxrule=0.5pt]
	\begin{verbatim}
|-- main.py
|-- output
|   `-- reportes ...
|-- test
|   `-- textos de prueba ...
`-- tokenizer
    |-- AFDTokenizer.py
    |-- analisis.py
    |-- HashTokenizer.py
    |-- sentiment_symbols
    |   |-- despedidas.txt
    |   |-- identificaciones.txt
    |   |-- palabras_prohibidas.txt
    |   |-- palabras_y_puntajes.txt
    |   `-- saludos.txt
    |-- TablaSentimientos.py
    `-- tokens.py
\end{verbatim}
\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    La tabla de sentimientos     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Tabla de Sentimientos}
\texttt{TablaSentimientos} es la clase encargada de almacenar, gestionar y consultar
diferentes tipos de palabras utilizadas durante el análisis de sentimiento. Para ello, carga en
memoria cinco diccionarios distintos desde archivos persistentes: palabras con puntaje,
saludos, despedidas, identificaciones y palabras prohibidas, a través del método \texttt{\_cargar\_datos()}.

Permite agregar o eliminar palabras con su puntaje asociado mediante los métodos
\texttt{agregar\_palabra(palabra: str, puntuacion: int)} y \texttt{eliminar\_palabra(palabra: str)}.

Provee un método de búsqueda, \texttt{buscar\_palabra(palabra: str)}, el cual retorna el tipo
de palabra (saludo, prohibida, desconocida, etc) y su puntuación de sentimiento.

La clase también ofrece un sistema de sugerencias basado en la similitud de cadenas
(\texttt{difflib.get\_close\_matches}), accesible mediante el método
\texttt{sugerir\_similares()}, facilitando así la corrección y actualización interactiva de la
tabla de sentimientos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%      Los tokenizadores        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tokenizacion}

%%%%%  Tokenizador AFD  %%%%%
\subsubsection{Tokenizador AFD}

La clase \texttt{AFDTokenizer} transforma un texto de entrada en una secuencia de tokens
semánticamente significativos. Para ello, utiliza un autómata finito determinista (AFD)
generado a partir de una instancia de \texttt{TablaSentimientos}, y aplica un algoritmo de
\textit{longest match} para asegurar coincidencias máximas.

\subsubsection{Construcción del AFD}

El AFD se inicializa mediante el método \texttt{\_build\_afd\_completo()}, que crea un estado
inicial denominado \texttt{start} y lo extiende agregando transiciones según los distintos
tipos de frases registrados en \texttt{TablaSentimientos}. Estas categorías incluyen:

\begin{itemize}
	\item Palabras con puntaje (\texttt{self.tabla.palabras})
	\item Saludos
	\item Despedidas
	\item Identificaciones
	\item Palabras prohibidas
\end{itemize}

La incorporación se realiza con \texttt{\_cargar\_frases\_al\_afd()}, que descompone cada frase
carácter por carácter. Se generan estados intermedios hasta llegar al último carácter, cuyo
estado asociado se marca como final. En dicho estado final se almacenan el tipo de token y, si
corresponde, el puntaje asociado.

\paragraph{\footnotesize{Representación del Estado de Error:}}
El estado de \textit{error}, el cual captura palabras ajenas al vocabulario del AFD, se
implementa implícitamente mediante transiciones no definidas. Cuando un símbolo de entrada no
tiene transición asociada desde el estado actual, el sistema interpreta automáticamente una
transición al estado de error. Esta estrategia optimiza memoria (al evitar representar
transiciones redundantes) y simplifica la lógica de implementación. Esto equivale a modelar
$q_{\text{error}}$ como sumidero universal no almacenado explícitamente.

\subsubsection{Persistencia del AFD}
El método \texttt{\_persistir\_afd()} guarda la estructura resultante en formato JSON bajo la
carpeta \texttt{output}, facilitando su inspección. Cabe denotar que, a fin de simplificar la
implementación, dicha representación no es utilizada para reconstruir el AFD.

\subsubsection{Preprocesamiento del texto}
Antes de tokenizar, se aplica \texttt{\_preprocesar\_hablantes()} para normalizar marcas como
\texttt{agente:} o \texttt{cliente:}, asegurando que sean detectadas como unidades separadas.
Luego, \texttt{\_procesar\_hablante()} se encarga de convertir estas marcas en tokens
especializados.

\subsubsection{Tokenización con AFD y \textit{longest match}}
El núcleo del análisis léxico ocurre en \texttt{tokenizar()}, que recorre el texto carácter por
carácter. Cuando se identifica una palabra candidata, se invoca
\texttt{\_tokenizar\_con\_afd()}.

Este método implementa un algoritmo de \textbf{búsqueda por mayor coincidencia}
(\textit{longest match}) sobre el AFD. A partir de la posición actual, se simulan todas las
transiciones posibles en paralelo:

\begin{itemize}
	\item Cada vez que se alcanza un estado final, se guarda un token candidato.
	\item Se prioriza el token que consuma la mayor cantidad de caracteres consecutivos desde la posición inicial.
	\item Si hay múltiples candidatos válidos, se conserva sólo el más largo.
\end{itemize}

Esto permite resolver ambigüedades como \textit{``buen''}, \textit{``buenos''} y \textit{``buen
	día''}.

\subsubsection{Fallback: \texttt{\_tokenizar\_palabra\_simple()}}
Si \texttt{\_tokenizar\_con\_afd()} no logra reconocer ningún token válido, se recurre al
método \texttt{\_tokenizar\_palabra\_simple()}. Este método agrupa letras consecutivas para
formar una palabra y consulta \texttt{TablaSentimientos.buscar\_palabra()}. Si la palabra no
está registrada, se clasifica como \texttt{DESCONOCIDO}.

\subsubsection{Manejo de signos de puntuación}
Cualquier carácter que no sea alfanumérico ni parte de una palabra compuesta se clasifica como
signo de puntuación mediante \texttt{\_es\_signo\_puntuacion()}, y se encapsula en un token con
tipo \texttt{TOKEN\_SIGNO\_PUNTUACION}.

\subsubsection{Resumen del flujo de análisis léxico}
\begin{enumerate}
	\item Se preprocesa el texto para separar marcas de hablante.
	\item Se recorre el texto carácter a carácter.
	\item Se tokenizan signos de puntuación y hablantes de forma directa.
	\item Se intenta reconocer frases usando el AFD y \texttt{\_tokenizar\_con\_afd()} con \textit{longest match}.
	\item Si el AFD falla, se analiza como palabra suelta con \texttt{\_tokenizar\_palabra\_simple()}.
\end{enumerate}

\subsubsection{Ventajas y limitaciones}
Entre sus principales ventajas encontramos que hace que el proceso de tokenización sea
determinista y predecible. Además, una vez construido, el AFD es especialmente eficiente en
entornos con grandes volúmenes de texto.

La principal desventaja radica en su complejidad, ya que la construcción del AFD puede implicar
un procesamiento costoso tanto en tiempo como en memoria. Además, mantener el AFD en memoria
durante la ejecución puede requerir una cantidad significativa de espacio, especialmente cuando
el conjunto de patrones es grande o altamente variable. Asimismo, la implementación resulta más
compleja de extender y mantener, particularmente cuando se ajustar los criterios de
tokenización.

%%%%%  Tokenizador hashmap  %%%%%
\subsection{Tokenizador Hashmap}
El tokenizador \texttt{HashTokenizer} ofrece una alternativa más simple y directa al uso de un
AFD. Su funcionamiento se basa en una exploración secuencial del texto con búsqueda de
coincidencias en un diccionario (la instancia de \texttt{TablaSentimientos}). Aunque menos
sofisticado que el AFD, su implementación es rápida y efectiva.

\subsubsection{Expresiones regulares}
Se emplean dos expresiones regulares precompiladas (lo que técnicamente convierte la
implementación en híbrida) para agilizar el preprocesamiento del texto:

\begin{itemize}
	\item \texttt{\_hablante\_re}: Detecta marcas de hablante como \texttt{agente:} o
	      \texttt{cliente:} para separarlas del resto del texto.

	\item \texttt{\_palabras\_re}: Segmenta el texto en unidades léxicas válidas. Detecta
	      palabras alfanuméricas y signos de puntuación como unidades separadas.
\end{itemize}

\subsubsection{Algoritmo de tokenización}
El método \texttt{tokenizar()} transforma el texto en una lista de tokens mediante los
siguientes pasos:

\begin{enumerate}
	\item Se normalizan las marcas de hablante agregando espacios alrededor de \texttt{agente:}
	      y \texttt{cliente:}.

	\item Se aplica la expresión regular para obtener una lista de palabras y signos.

	\item Se recorre la lista desde la izquierda aplicando una búsqueda decreciente de frases
	      de longitud 3, 2 y 1 palabras.
\end{enumerate}

\subsubsection{Manejo de marcas de hablante}
Cuando el patrón detecta secuencias como \texttt{agente :} o \texttt{cliente :}, se agrupan y
transforman en un token específico (\texttt{TOKEN\_AGENTE} o \texttt{TOKEN\_CLIENTE}). Este
paso se prioriza antes de intentar reconocer frases.

\subsubsection{Reconocimiento de frases por ventana deslizante}
En cada posición, se intenta formar una frase de longitud 3, luego 2, y finalmente 1 palabra.
Para cada combinación posible:

\begin{itemize}
	\item Se forma la frase concatenando las palabras con espacios.

	\item Se consulta \texttt{tabla.buscar\_palabra(frase)}.

	\item Si se encuentra una coincidencia, se genera un token con el tipo correspondiente y se
	      avanza el cursor \texttt{i} en la longitud de la frase.
\end{itemize}

Este mecanismo puede considerarse una versión simplificada del algoritmo de longest match, sin
necesidad de utilizar una estructura AFD. Sin embargo, también puede presentar errores al
detectar frases que contienen signos de puntuación.

\subsubsection{Fallback: palabra o signo aislado}
Si no se reconoce ninguna frase:

\begin{itemize}
	\item Si el fragmento es un carácter no alfanumérico (\texttt{\textbackslash W}), se
	      clasifica como \texttt{TOKEN\_SIGNO\_PUNTUACION}.

	\item Si es una palabra, se vuelve a consultar en la tabla de sentimientos como palabra
	      individual. Si no se encuentra, se clasifica como \texttt{DESCONOCIDO}.
\end{itemize}

\subsubsection{Ventajas y limitaciones}
Una de las principales ventajas de este enfoque es que su implementación es más directa,
comprensible y extensible. Además, presenta una mayor velocidad de instanciado, ya que no
requiere la construcción previa del AFD. También resulta razonablemente eficaz cuando las
frases significativas tienen una longitud acotada.

Entre sus limitaciones, se encuentra el hecho de que no garantiza coincidencias cuando las
frases contienen signos de puntuación y puede ser menos eficiente (en tiempo de CPU) cuando se
trabaja con grandes volúmenes de texto.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%      Analizador de sentimiento        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Análisis de sentimiento y protocolo de atención}
El análisis de sentimiento se basa en recorrer la secuencia de tokens de la conversación,
asignando puntajes y detectando eventos clave para cliente y agente.

Se distingue quién habla en cada momento y se acumulan las métricas asociadas. También se
identifican eventos importantes para evaluar el protocolo de atención.

\subsubsection{Protocolos y etiquetas especiales}
Se detectan eventos que afectan la evaluación del protocolo:

\begin{itemize}
	\item \textbf{Saludo}: apertura cordial.
	\item \textbf{Despedida}: cierre adecuado.
	\item \textbf{Identificación}: presentación del agente.
	\item \textbf{Palabras prohibidas}: lenguaje inapropiado.
\end{itemize}

Esto se realiza de forma independiente tanto para el agente como para el cliente.

\subsubsection{Estructura del procesamiento}
La función principal recorre los tokens con tres objetivos. Primero, identificar el hablante
actual (cliente o agente). Segundo, acumular puntajes y marcar eventos por hablante. Y tercero,
detectar palabras desconocidas para su posterior manejo.

\begin{verbatim}
def analizar_sentimiento(tokens, tabla_sentimientos):
    resultado = ResultadoConversacion(...)
    palabras_desconocidas = []
    hablante_actual = "agente"

    for token in tokens:
        if token.type == "TOKEN_CLIENTE":
            hablante_actual = "cliente"
            continue
        elif token.type == "TOKEN_AGENTE":
            hablante_actual = "agente"
            continue

        participante = getattr(resultado, hablante_actual)

        if token.type == TOKEN_PROHIBIDA:
            participante.hay_prohibidas = True
        elif token.type == TOKEN_SALUDO:
            participante.hay_saludo = True
        elif token.type == TOKEN_DESPEDIDA:
            participante.hay_despedida = True
        elif token.type == TOKEN_IDENTIFICACION:
            participante.hay_identificacion = True
        elif token.type == TOKEN_DESCONOCIDO:
            palabras_desconocidas.append((hablante_actual, token.valor))

        participante.puntaje_total += token.puntuacion
        resultado.puntaje_total += token.puntuacion
\end{verbatim}

\subsubsection{Corrección de tokens desconocidos}
Durante el análisis, las palabras no encontradas en la tabla de sentimientos se clasifican como
desconocidas. Para cada una se ofrece un menú interactivo con opciones:

\begin{itemize}
	\item \textbf{Agregar manualmente}: ingresar puntaje para agregar la palabra.
	\item \textbf{Corregir con sugerencia}: elegir palabra similar existente.
	\item \textbf{Ignorar}: no modificar el puntaje.
\end{itemize}

\begin{verbatim}
[a] Agregar palabra manualmente")
[c] Corregir usando una sugerencia")
[i] Ignorar palabra")
    Seleccione una opción:
\end{verbatim}

Si el usuario decide agregar o corregir, el puntaje se suma al hablante y a la conversación. Si
ignora, se registra la palabra para análisis futuro.

Este procedimiento permite mejorar y ajustar dinámicamente la tabla de sentimientos para
futuros análisis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%      Menu principal        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\textit{main.py} (informes y flujo completo)}

La generación de informes ocurre dentro de la función principal del archivo \textit{main.py},
que además gestiona el flujo completo de ejecución (desde la selección del tokenizador hasta la
escritura del informe final en la consola o en un archivo de salida).

Existen dos formatos principales de salida. Si el programa se ejecuta en modo interactivo (no
se proporciona un archivo de entrada), entonces se produce un informe simplificado. Este
formato está pensado para exploraciones rápidas o pruebas manuales. Por otro lado, si se
proporciona un archivo de texto para analizar, se realiza un informe más detallado.

El sistema también diferencia entre la salida por consola (donde se utilizan códigos ANSI para
resaltar visualmente los distintos tipos de tokens) y la salida a archivo, que se imprime sin
colores para facilitar su posterior procesamiento o revisión.
